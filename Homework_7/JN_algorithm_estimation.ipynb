{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f49d9d2-7147-4718-90f8-845b322bd495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from math import ceil\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c94c830-b381-485a-a8c5-3c1f02fc9833",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b54979-54b6-41ea-8827-cf1c1e715414",
   "metadata": {},
   "source": [
    "## Frequency stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a964854-5dae-49b0-88df-ccd766bf3fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_period(df_arg, column_name_arg):\n",
    "    \"\"\"Function for finding the average value of the measurement period\n",
    "    :param df_arg: researched dataframe\n",
    "    :param column_name_arg: the name of the column that contains information about the time of the measurements\n",
    "    (e.g., the 'time' column)\n",
    "    :return: the average value of the measurement period\n",
    "    \"\"\"\n",
    "    return df_arg[column_name_arg].diff().mean()\n",
    "\n",
    "\n",
    "def get_avg_frequency(df_arg, column_name_arg):\n",
    "    \"\"\"Function for finding the average value of the measurement frequency\n",
    "        :param df_arg: researched dataframe\n",
    "        :param column_name_arg: the name of the column that contains information about the time of the measurements\n",
    "        (e.g., the 'time' column)\n",
    "        :return: the average value of the measurement frequency\n",
    "        \"\"\"\n",
    "    time_diffs = df_arg[column_name_arg].diff()\n",
    "    return 1.0 / time_diffs.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec526c28-70c6-4cdc-8400-9c1f14b78825",
   "metadata": {},
   "source": [
    "## Data Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cd684b3-1643-4da6-86ee-2064d62b0def",
   "metadata": {},
   "outputs": [],
   "source": [
    "def median_filter_data(df_arg, filter_columns_arg, window_size_arg):\n",
    "    \"\"\"Function to filter data using a median filter with a selected window of the specified columns of the transmitted dataframe\n",
    "    :param df_arg: the dataframe whose column contents you want to filter\n",
    "    :param filter_columns_arg: a list of columns whose contents should be filtered\n",
    "    :param window_size_arg: window size for the median filter\n",
    "    :return: a dataframe that contains the filtered column values\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    for column in filter_columns_arg:\n",
    "        # Apply median filtering to column data\n",
    "        df[f'{column}_filtered'] = df_arg[column].rolling(window=window_size_arg, center=True, min_periods=1).median()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8f26e5-f7f8-42af-bd4d-c6161fb7da95",
   "metadata": {},
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28e1bc57-bb55-4b07-a3cd-c4857a7d2415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_undersampled_df(df_arg, column_name_arg):\n",
    "    # First, calculate the minimum number of samples across all classes\n",
    "    min_samples = df_arg[column_name_arg].value_counts().min()\n",
    "\n",
    "    # Initialize an empty DataFrame to store the undersampled data\n",
    "    undersampled_df = pd.DataFrame()\n",
    "\n",
    "    # Loop through each unique activity class and select the first min_samples for each class\n",
    "    for activity_class in df_arg[column_name_arg].unique():\n",
    "        class_subset = df_arg[df_arg[column_name_arg] == activity_class].iloc[:min_samples]\n",
    "        undersampled_df = pd.concat([undersampled_df, class_subset])\n",
    "\n",
    "    return undersampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af6631c9-97f8-4e24-831c-76296f636dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discard_columns(corr_matrix_arg, important_columns_arg, df_arg):\n",
    "\n",
    "    columns_to_discard = set()\n",
    "    for column in corr_matrix_arg.columns:\n",
    "        correlated_columns = corr_matrix_arg.index[\n",
    "            (corr_matrix_arg[column] > 0.5) | (corr_matrix_arg[column] < -0.5)\n",
    "            ]\n",
    "\n",
    "        for correlated_column in correlated_columns:\n",
    "            if column != correlated_column:\n",
    "                # Prioritize which column to keep based on your criteria\n",
    "                # For example, keep the column with higher variance\n",
    "                if column not in important_columns_arg:\n",
    "                    columns_to_discard.add(column)\n",
    "                elif (column in important_columns_arg) and (correlated_column in important_columns_arg):\n",
    "                    pass\n",
    "                elif (column in important_columns_arg) and (correlated_column not in important_columns_arg):\n",
    "                    columns_to_discard.add(correlated_column)\n",
    "                else:  # both columns are not in important_columns\n",
    "                    columns_to_discard.add(\n",
    "                        correlated_column if df_arg[correlated_column].var() < df_arg[column].var() else column)\n",
    "\n",
    "    return columns_to_discard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdcb7eb-80e2-44d5-b3c7-95276541ab00",
   "metadata": {},
   "source": [
    "## Windowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff706c35-2026-45c3-8741-d15adcf8bd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_windowed_df(df_arg, window_duration_arg):\n",
    "    # Calculate the number of data points within a 2-second window\n",
    "    sampling_frequency = 1.0 / df_arg['time'].diff().mean()  # Hz\n",
    "    window_size = ceil(sampling_frequency * window_duration_arg)\n",
    "    step_size = window_size // 2\n",
    "\n",
    "    # Create a list to store the windowed dataframes\n",
    "    windowed_dfs = []\n",
    "\n",
    "    windowed_dict = {'accX': [], 'accY': [], 'accZ': [], 'gyrZ': [], 'activity': []}\n",
    "    # Divide the entire dataframe into 2-second windows\n",
    "    for i in range(0, len(df_arg), step_size):\n",
    "        window_df = df_arg.iloc[i:i + window_size]\n",
    "\n",
    "        windowed_dict['accX'].append(window_df['accX_filtered'].values)\n",
    "        windowed_dict['accY'].append(window_df['accY_filtered'].values)\n",
    "        windowed_dict['accZ'].append(window_df['accZ_filtered'].values)\n",
    "\n",
    "        windowed_dict['gyrZ'].append(window_df['gyrZ_filtered'].values)\n",
    "\n",
    "        # Determine the most frequent activity in the window\n",
    "        most_frequent_activity = window_df['activity'].value_counts().idxmax()\n",
    "        # Assign the most frequent activity to all rows in the window\n",
    "        windowed_dict['activity'].append(most_frequent_activity)\n",
    "\n",
    "        windowed_dfs.append(window_df)\n",
    "\n",
    "    return pd.DataFrame.from_dict(windowed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb7d107-9e9f-46c3-9e5b-75b9c176880b",
   "metadata": {},
   "source": [
    "## Feature_engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a9ff0f6-254b-40ff-a5c3-ec20f8858409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statistical_measures_df(windowed_data_df, functions, data_df_columns, result_df_columns):\n",
    "    \"\"\"Function for creating a dataframe X_df, the columns of which correspond to the required statistical measures for\n",
    "    the windows of the windowed_data_df dataframe\n",
    "    :param windowed_data_df: a dataframe whose rows contain arrays of data formed as a result of windowing\n",
    "    :param functions: a list of references to lambda functions that will calculate the required statistical measures\n",
    "    :param data_df_columns: a list of column names of the windowed_data_df dataframe for which to find statistical\n",
    "    measures\n",
    "    :param result_df_columns: a list of names of the searched statistical measures of the output dataframe\n",
    "    :return: X_train dataframe\n",
    "    An example of using the function:\n",
    "        Let's imagine we have a windowed_data_df dataframe (a dataframe formed as a result of windowing - that is, each record in a row can be an array of records for a given window), which contains the columns 'accX', 'accY', 'accZ' (the results of measuring the readings of the accelerometer on the corresponding axes).\n",
    "        Inside the function, we create an X_df dataframe to which we want to add new columns with statistical measure values.\n",
    "        For example, we want to calculate the statistical mean and avg absolute diff for the accelerometer readings on all three axes (the columns 'accX', 'accY', 'accZ' of data_df), so the columns of the X_df dataframe will be named, for example, 'accX_mean', 'accY_mean', ..., 'accZ_aad'.\n",
    "        These values can be calculated using the lambda functions: [\n",
    "            lambda x: x.mean(),\n",
    "            lambda x: np.mean(np.absolute(x - np.mean(x)))\n",
    "        ]\n",
    "        So, the function call will look like this:\n",
    "        get_statistical_measures_df(df=data_df,\n",
    "                                    functions=[\n",
    "                                        lambda x: x.mean(),\n",
    "                                        lambda x: np.mean(np.absolute(x - np.mean(x)))\n",
    "                                    ],\n",
    "                                    data_df_columns=['accX', 'accY', 'accZ'],\n",
    "                                    result_df_columns=['mean', 'aad'])\n",
    "    \"\"\"\n",
    "    X_df = pd.DataFrame()\n",
    "    for [function, res_column] in zip(functions, result_df_columns):\n",
    "        for data_column in data_df_columns:\n",
    "            X_df[f'{data_column}_{res_column}'] = windowed_data_df[data_column].apply(function)\n",
    "    return X_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c4af70c-7f89-4d74-b22e-4fbab10e5ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_feature_engineering(df_arg, functions_list):\n",
    "    y_train = df_arg['activity'].values\n",
    "    result_columns = ['mean', 'std', 'aad', 'min', 'max', 'range', 'median', 'iqr', 'neg_count', 'pos_count',\n",
    "                      'assymetry', 'kurtosis']\n",
    "    df_arg = get_statistical_measures_df(windowed_data_df=df_arg,\n",
    "                                                             functions=functions_list,\n",
    "                                                             data_df_columns=['accX', 'accY', 'accZ', 'gyrZ'],\n",
    "                                                             result_df_columns=result_columns)\n",
    "    df_arg['activity'] = y_train\n",
    "    return df_arg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9518590c-e19a-45bc-82d2-c97cf8cadc3f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Training Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72256ee9-49c2-4413-8e20-d75d887f7c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_data(train_df_arg, training_part=0.8):\n",
    "    X_train = pd.DataFrame()\n",
    "    y_train = []\n",
    "\n",
    "    X_valid = pd.DataFrame()\n",
    "    y_valid = []\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    for activity in train_df_arg['activity'].unique():\n",
    "        activity_data = train_df_arg[train_df_arg['activity'] == activity].copy()\n",
    "        activity_data.reset_index(inplace=True)\n",
    "        activity_data.drop('index', axis=1, inplace=True)\n",
    "        split_index = int(training_part * len(activity_data))\n",
    "        if counter != 0:\n",
    "            X_train = pd.concat([X_train, activity_data[activity_data.columns[:-2]][:split_index]])\n",
    "            X_valid = pd.concat([X_valid, activity_data[activity_data.columns[:-2]][split_index:]])\n",
    "        else:\n",
    "            X_train = activity_data[activity_data.columns[:-2]][:split_index]\n",
    "            X_valid = activity_data[activity_data.columns[:-2]][split_index:]\n",
    "\n",
    "        y_train.extend(list(activity_data['activity_number'].values[:split_index]))\n",
    "        y_valid.extend(list(activity_data['activity_number'].values[split_index:]))\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "    return [X_train, y_train, X_valid, y_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02fca059-a04a-4017-8f40-59bfaf69d084",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_target_features(y_arg, one_hot_encoding):\n",
    "    y_arg = np.array(y_arg)\n",
    "    if one_hot_encoding:\n",
    "        # Convert Label Encoded target data to one-hot encoded format\n",
    "        y_arg = to_categorical(y_arg)\n",
    "    return y_arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9449619-d30c-4abe-82cd-556e4b73de16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training_data_preparation(df_arg):\n",
    "    # Convert string labels to int\n",
    "    activity_dict = {'Squat': 0, 'Leg land': 1, 'Walk': 2, 'Lateral squat slide': 3, 'Jogging': 4}\n",
    "    df_arg['activity_number'] = df_arg['activity'].apply(lambda x: activity_dict[x])\n",
    "\n",
    "    X_train, y_train, X_valid, y_valid = split_train_data(train_df_arg=df_arg)\n",
    "    y_train = prepare_target_features(y_arg=y_train, one_hot_encoding=True)\n",
    "    y_valid = prepare_target_features(y_arg=y_valid, one_hot_encoding=True)\n",
    "    # Scale feature vectors\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_valid = scaler.transform(X_valid)\n",
    "    return X_train, y_train, X_valid, y_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54eb971-4aeb-4a72-bd68-cb44676ad3d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Pipeline functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcf3c11-d667-4f55-9f91-ba378e286e81",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Original pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6dbe426-3c3e-4164-a1dc-1861e83e370c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_pipeline(df_arg):\n",
    "    # Exploring measurement period and frequency stability\n",
    "    df_arg = df_arg[df_arg['time'].diff() <= get_avg_period(df_arg, 'time') * 1.5]\n",
    "\n",
    "    # Data Filtering\n",
    "    df_arg[['accX_filtered', 'accY_filtered', 'accZ_filtered', 'gyrX_filtered', 'gyrY_filtered', 'gyrZ_filtered']] \\\n",
    "        = median_filter_data(df_arg=df_arg,\n",
    "                             filter_columns_arg=['accX', 'accY', 'accZ', 'gyrX', 'gyrY', 'gyrZ'],\n",
    "                             window_size_arg=10)\n",
    "\n",
    "    # Exploratory Data Analysis\n",
    "    df_arg = df_arg[df_arg['activity'] != 'No activity']\n",
    "\n",
    "    # Perform undersampling to get a balanced dataframe\n",
    "    df_arg = get_undersampled_df(df_arg=df_arg, column_name_arg='activity')\n",
    "\n",
    "    # Build a correlation matrix and remove certain axes of the accelerometer or gyroscope\n",
    "    sel_columns = ['accX_filtered', 'accY_filtered', 'accZ_filtered', 'gyrX_filtered', 'gyrY_filtered',\n",
    "                   'gyrZ_filtered']\n",
    "\n",
    "    # Calculate the correlation matrix for the selected columns\n",
    "    corr_matrix = df_arg[sel_columns].corr()\n",
    "    important_columns = ['accX_filtered', 'accY_filtered', 'accZ_filtered']\n",
    "    discard_columns = get_discard_columns(corr_matrix_arg=corr_matrix,\n",
    "                                          important_columns_arg=important_columns,\n",
    "                                          df_arg=df_arg)\n",
    "    sel_columns = [col for col in sel_columns if col not in discard_columns]\n",
    "    sel_columns.append('activity')\n",
    "    filtered_df = df_arg[['time'] + sel_columns].copy()\n",
    "\n",
    "    # Windowing\n",
    "    windowed_df = get_windowed_df(df_arg=filtered_df, window_duration_arg=2)\n",
    "\n",
    "    # Feature Engineering\n",
    "    functions_list = [\n",
    "        lambda x: x.mean(),  # mean\n",
    "        lambda x: x.std(),  # std deviation\n",
    "        lambda x: np.mean(np.absolute(x - np.mean(x))),  # avg absolute diff\n",
    "        lambda x: x.min(),  # min\n",
    "        lambda x: x.max(),  # max\n",
    "        lambda x: x.max() - x.min(),  # range = max-min diff\n",
    "        lambda x: np.median(x),  # median\n",
    "        lambda x: np.percentile(x, 75) - np.percentile(x, 25),  # interquartile range\n",
    "        lambda x: np.sum(x < 0),  # negative count\n",
    "        lambda x: np.sum(x > 0),  # positive count\n",
    "        lambda x: stats.skew(x),  # skewness = assymetry\n",
    "        lambda x: stats.kurtosis(x)  # kurtosis\n",
    "    ]\n",
    "    windowed_df = perform_feature_engineering(df_arg=windowed_df, functions_list=functions_list)\n",
    "\n",
    "    # Model training\n",
    "    return model_training_data_preparation(df_arg=windowed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf7191e-0d44-4b9e-89be-22271f807e44",
   "metadata": {},
   "source": [
    "## Estimation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "157b9fe9-32a3-4030-b2c1-d9d363d6ac4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_usage(df_arg):\n",
    "    # Measure start time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Call the pipeline function\n",
    "    X_train, y_train, X_valid, y_valid = perform_pipeline(df_arg=df_arg)\n",
    "\n",
    "    # Measure end time\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bf39997-26f6-4fe1-b1ee-a4f693cb3641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimized_time_usage(df_arg):\n",
    "    # Measure start time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Call the pipeline function\n",
    "    X_train, y_train, X_valid, y_valid = perform_optimized_pipeline(df_arg=df_arg)\n",
    "\n",
    "    # Measure end time\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "318e39d0-902f-44ca-ba89-2678c5e9e0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_usage(df_arg, pipeline_func):\n",
    "    process = psutil.Process()\n",
    "    \n",
    "    # Measure memory before running the pipeline\n",
    "    mem_before = process.memory_info().rss\n",
    "\n",
    "    # Call the pipeline function\n",
    "    pipeline_func(df_arg)\n",
    "\n",
    "    # Measure memory after running the pipeline\n",
    "    mem_after = process.memory_info().rss\n",
    "    mem_used = mem_after - mem_before\n",
    "    return mem_used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e69eb7a-bf8f-461f-b09c-4d807d165668",
   "metadata": {},
   "source": [
    "# Work with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f0c2f71-f893-4c0d-bc3d-0641d0d45d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 63529 entries, 0 to 63528\n",
      "Data columns (total 9 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   timestamp  63529 non-null  int64  \n",
      " 1   time       63529 non-null  float64\n",
      " 2   accX       63529 non-null  float64\n",
      " 3   accY       63529 non-null  float64\n",
      " 4   accZ       63529 non-null  float64\n",
      " 5   gyrX       63529 non-null  float64\n",
      " 6   gyrY       63529 non-null  float64\n",
      " 7   gyrZ       63529 non-null  float64\n",
      " 8   activity   63529 non-null  object \n",
      "dtypes: float64(7), int64(1), object(1)\n",
      "memory usage: 4.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/Train/Train_activities_1_2023-08-23.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d40e3e4-7c65-45dc-b69a-fd757fdc5324",
   "metadata": {},
   "source": [
    "### Investigate original pipeline execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "956a061f-4e59-4cba-a76f-29b965680168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original pipeline\n",
      "1) time = 7.071 seconds\n",
      "2) time = 6.937 seconds\n",
      "3) time = 7.076 seconds\n",
      "4) time = 6.956 seconds\n",
      "5) time = 7.086 seconds\n",
      "average execution time =  7.025 seconds\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original pipeline\")\n",
    "number_of_experiments = 5\n",
    "time_list = []\n",
    "for i in range(number_of_experiments):\n",
    "    temp_time = get_time_usage(df_arg=df)\n",
    "    time_list.append(temp_time)\n",
    "    print(f\"{i+1}) time = {temp_time:.3f} seconds\")\n",
    "    \n",
    "print(f\"average execution time = {sum(time_list) / len(time_list): .3f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
