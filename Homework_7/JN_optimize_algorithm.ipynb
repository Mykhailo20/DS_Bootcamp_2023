{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f49d9d2-7147-4718-90f8-845b322bd495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from math import ceil\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c94c830-b381-485a-a8c5-3c1f02fc9833",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec526c28-70c6-4cdc-8400-9c1f14b78825",
   "metadata": {},
   "source": [
    "## Data Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cd684b3-1643-4da6-86ee-2064d62b0def",
   "metadata": {},
   "outputs": [],
   "source": [
    "def median_filter_data(df_arg, filter_columns_arg, window_size_arg):\n",
    "    \"\"\"Function to filter data using a median filter with a selected window of the specified columns of the transmitted dataframe\n",
    "    :param df_arg: the dataframe whose column contents you want to filter\n",
    "    :param filter_columns_arg: a list of columns whose contents should be filtered\n",
    "    :param window_size_arg: window size for the median filter\n",
    "    :return: a dataframe that contains the filtered column values\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    for column in filter_columns_arg:\n",
    "        # Apply median filtering to column data\n",
    "        df[f'{column}_filtered'] = df_arg[column].rolling(window=window_size_arg, center=True, min_periods=1).median()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8f26e5-f7f8-42af-bd4d-c6161fb7da95",
   "metadata": {},
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28e1bc57-bb55-4b07-a3cd-c4857a7d2415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_undersampled_df(df_arg, column_name_arg):\n",
    "    # First, calculate the minimum number of samples across all classes\n",
    "    min_samples = df_arg[column_name_arg].value_counts().min()\n",
    "\n",
    "    # Initialize an empty DataFrame to store the undersampled data\n",
    "    undersampled_df = pd.DataFrame()\n",
    "\n",
    "    # Loop through each unique activity class and select the first min_samples for each class\n",
    "    for activity_class in df_arg[column_name_arg].unique():\n",
    "        class_subset = df_arg[df_arg[column_name_arg] == activity_class].iloc[:min_samples]\n",
    "        undersampled_df = pd.concat([undersampled_df, class_subset])\n",
    "\n",
    "    return undersampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af6631c9-97f8-4e24-831c-76296f636dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discard_columns(corr_matrix_arg, important_columns_arg, df_arg):\n",
    "\n",
    "    columns_to_discard = set()\n",
    "    for column in corr_matrix_arg.columns:\n",
    "        correlated_columns = corr_matrix_arg.index[\n",
    "            (corr_matrix_arg[column] > 0.5) | (corr_matrix_arg[column] < -0.5)\n",
    "            ]\n",
    "\n",
    "        for correlated_column in correlated_columns:\n",
    "            if column != correlated_column:\n",
    "                # Prioritize which column to keep based on your criteria\n",
    "                # For example, keep the column with higher variance\n",
    "                if column not in important_columns_arg:\n",
    "                    columns_to_discard.add(column)\n",
    "                elif (column in important_columns_arg) and (correlated_column in important_columns_arg):\n",
    "                    pass\n",
    "                elif (column in important_columns_arg) and (correlated_column not in important_columns_arg):\n",
    "                    columns_to_discard.add(correlated_column)\n",
    "                else:  # both columns are not in important_columns\n",
    "                    columns_to_discard.add(\n",
    "                        correlated_column if df_arg[correlated_column].var() < df_arg[column].var() else column)\n",
    "\n",
    "    return columns_to_discard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdcb7eb-80e2-44d5-b3c7-95276541ab00",
   "metadata": {},
   "source": [
    "## Windowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff706c35-2026-45c3-8741-d15adcf8bd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_windowed_df(df_arg, window_duration_arg, columns, activity_col, time_column='time'):\n",
    "    # Calculate the number of data points within a 2-second window\n",
    "    sampling_frequency = 1.0 / df_arg[time_column].diff().mean()  # Hz\n",
    "    window_size = ceil(sampling_frequency * window_duration_arg)\n",
    "    step_size = window_size // 2\n",
    "\n",
    "    # Create a list to store the windowed dataframes\n",
    "    windowed_dfs = []\n",
    "\n",
    "    windowed_dict = {col: [] for col in columns + [activity_col]}\n",
    "\n",
    "    # Divide the entire dataframe into 2-second windows\n",
    "    for i in range(0, len(df_arg), step_size):\n",
    "        window_df = df_arg.iloc[i:i + window_size]\n",
    "\n",
    "        for col in columns:\n",
    "            windowed_dict[col].append(window_df[col].values)\n",
    "\n",
    "        # Determine the most frequent activity in the window\n",
    "        most_frequent_activity = window_df[activity_col].value_counts().idxmax()\n",
    "        # Assign the most frequent activity to all rows in the window\n",
    "        windowed_dict[activity_col].append(most_frequent_activity)\n",
    "\n",
    "        windowed_dfs.append(window_df)\n",
    "\n",
    "    return pd.DataFrame.from_dict(windowed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb7d107-9e9f-46c3-9e5b-75b9c176880b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Feature_engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a9ff0f6-254b-40ff-a5c3-ec20f8858409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statistical_measures_df(windowed_data_df, functions, data_df_columns, result_df_columns):\n",
    "    \"\"\"Function for creating a dataframe X_df, the columns of which correspond to the required statistical measures for\n",
    "    the windows of the windowed_data_df dataframe\n",
    "    :param windowed_data_df: a dataframe whose rows contain arrays of data formed as a result of windowing\n",
    "    :param functions: a list of references to lambda functions that will calculate the required statistical measures\n",
    "    :param data_df_columns: a list of column names of the windowed_data_df dataframe for which to find statistical\n",
    "    measures\n",
    "    :param result_df_columns: a list of names of the searched statistical measures of the output dataframe\n",
    "    :return: X_train dataframe\n",
    "    An example of using the function:\n",
    "        Let's imagine we have a windowed_data_df dataframe (a dataframe formed as a result of windowing - that is, each record in a row can be an array of records for a given window), which contains the columns 'accX', 'accY', 'accZ' (the results of measuring the readings of the accelerometer on the corresponding axes).\n",
    "        Inside the function, we create an X_df dataframe to which we want to add new columns with statistical measure values.\n",
    "        For example, we want to calculate the statistical mean and avg absolute diff for the accelerometer readings on all three axes (the columns 'accX', 'accY', 'accZ' of data_df), so the columns of the X_df dataframe will be named, for example, 'accX_mean', 'accY_mean', ..., 'accZ_aad'.\n",
    "        These values can be calculated using the lambda functions: [\n",
    "            lambda x: x.mean(),\n",
    "            lambda x: np.mean(np.absolute(x - np.mean(x)))\n",
    "        ]\n",
    "        So, the function call will look like this:\n",
    "        get_statistical_measures_df(df=data_df,\n",
    "                                    functions=[\n",
    "                                        lambda x: x.mean(),\n",
    "                                        lambda x: np.mean(np.absolute(x - np.mean(x)))\n",
    "                                    ],\n",
    "                                    data_df_columns=['accX', 'accY', 'accZ'],\n",
    "                                    result_df_columns=['mean', 'aad'])\n",
    "    \"\"\"\n",
    "    X_df = pd.DataFrame()\n",
    "    for [function, res_column] in zip(functions, result_df_columns):\n",
    "        for data_column in data_df_columns:\n",
    "            X_df[f'{data_column}_{res_column}'] = windowed_data_df[data_column].apply(function)\n",
    "    return X_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9518590c-e19a-45bc-82d2-c97cf8cadc3f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Training Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "72256ee9-49c2-4413-8e20-d75d887f7c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_data(train_df_arg, training_part=0.8):\n",
    "    X_train = pd.DataFrame()\n",
    "    y_train = []\n",
    "\n",
    "    X_valid = pd.DataFrame()\n",
    "    y_valid = []\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    for activity in train_df_arg['activity'].unique():\n",
    "        activity_data = train_df_arg[train_df_arg['activity'] == activity].copy()\n",
    "        activity_data.reset_index(inplace=True)\n",
    "        activity_data.drop('index', axis=1, inplace=True)\n",
    "        split_index = int(training_part * len(activity_data))\n",
    "        if counter != 0:\n",
    "            X_train = pd.concat([X_train, activity_data[activity_data.columns[:-2]][:split_index]])\n",
    "            X_valid = pd.concat([X_valid, activity_data[activity_data.columns[:-2]][split_index:]])\n",
    "        else:\n",
    "            X_train = activity_data[activity_data.columns[:-2]][:split_index]\n",
    "            X_valid = activity_data[activity_data.columns[:-2]][split_index:]\n",
    "\n",
    "        y_train.extend(list(activity_data['activity_number'].values[:split_index]))\n",
    "        y_valid.extend(list(activity_data['activity_number'].values[split_index:]))\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "    return [X_train, y_train, X_valid, y_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "60c41a15-7114-451e-b547-c4210cd53e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(train_df_arg, test_df_arg, training_part=0.8):\n",
    "    X_train, y_train, X_valid, y_valid = split_train_data(train_df_arg=train_df_arg)\n",
    "    \n",
    "    X_test = test_df_arg[test_df_arg.columns[:-2]]\n",
    "    y_test = test_df_arg['activity_number'].values\n",
    "    \n",
    "    return [X_train, y_train, X_valid, y_valid, X_test, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a9449619-d30c-4abe-82cd-556e4b73de16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_data_preparation(df_arg, test_df_arg, activity_dict):\n",
    "    # Convert string labels to int\n",
    "    df_arg['activity_number'] = df_arg['activity'].apply(lambda x: activity_dict[x])\n",
    "    test_df_arg['activity_number'] = test_df_arg['activity'].apply(lambda x: activity_dict[x])\n",
    "    \n",
    "    X_train, y_train, X_valid, y_valid, X_test, y_test = split_data(train_df_arg=df_arg, test_df_arg=test_df_arg)\n",
    "    y_train = np.array(y_train)\n",
    "    y_valid = np.array(y_valid)\n",
    "    y_test = np.array(y_test)\n",
    "    \n",
    "    # Convert Label Encoded target data to one-hot encoded format\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_valid = to_categorical(y_valid)\n",
    "    y_test = to_categorical(y_test)\n",
    "    \n",
    "    # Scale feature vectors\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_valid = scaler.transform(X_valid)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    return X_train, y_train, X_valid, y_valid, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54eb971-4aeb-4a72-bd68-cb44676ad3d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Pipeline functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0eca835-b1f3-43cf-8b9f-419a0005be3c",
   "metadata": {},
   "source": [
    "## Optimized pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fbea0818-8324-4b71-97f9-079eb6c781d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_pipeline(df_arg, test_df_arg, activity_dict, data_columns, time_column, important_columns=[]):\n",
    "    \n",
    "    # Exploring measurement period and frequency stability\n",
    "    df_arg = df_arg[df_arg[time_column].diff() <= df_arg[time_column].diff().mean() * 1.5]\n",
    "\n",
    "    # Data Filtering\n",
    "    df_arg[data_columns] = median_filter_data(df_arg=df_arg,\n",
    "                                              filter_columns_arg=data_columns,\n",
    "                                              window_size_arg=10)\n",
    "    test_df_arg[data_columns] = median_filter_data(df_arg=test_df_arg,\n",
    "                                                   filter_columns_arg=data_columns,\n",
    "                                                   window_size_arg=10)\n",
    "    \n",
    "    # Exploratory Data Analysis\n",
    "    df_arg = df_arg[df_arg['activity'] != 'No activity']\n",
    "    test_df_arg = test_df_arg[test_df_arg['activity'] != 'No activity']\n",
    "    \n",
    "    # Undersample the training dataset to obtain a balanced dataframe\n",
    "    df_arg = get_undersampled_df(df_arg=df_arg, column_name_arg='activity')\n",
    "\n",
    "    # Build a correlation matrix and remove certain axes of the accelerometer or gyroscope\n",
    "    sel_columns = data_columns\n",
    "\n",
    "    # Calculate the correlation matrix for the selected columns\n",
    "    corr_matrix = df_arg[sel_columns].corr()\n",
    "    discard_columns = get_discard_columns(corr_matrix_arg=corr_matrix,\n",
    "                                          important_columns_arg=important_columns,\n",
    "                                          df_arg=df_arg)\n",
    "    sel_columns = [col for col in sel_columns if col not in discard_columns]\n",
    "    df_arg = df_arg[[time_column] + sel_columns + ['activity']].copy()\n",
    "    \n",
    "    # Windowing\n",
    "    df_arg = get_windowed_df(df_arg=df_arg, window_duration_arg=2, columns=sel_columns, activity_col='activity', time_column=time_column)\n",
    "    test_df_arg = get_windowed_df(df_arg=test_df_arg, window_duration_arg=2, columns=sel_columns, activity_col='activity', time_column=time_column)\n",
    "    \n",
    "    # Feature Engineering\n",
    "    functions_list = [\n",
    "        lambda x: x.mean(),  # mean\n",
    "        lambda x: x.std(),  # std deviation\n",
    "        lambda x: np.mean(np.absolute(x - np.mean(x))),  # avg absolute diff\n",
    "        lambda x: x.min(),  # min\n",
    "        lambda x: x.max(),  # max\n",
    "        lambda x: x.max() - x.min(),  # range = max-min diff\n",
    "        lambda x: np.median(x),  # median\n",
    "        lambda x: np.percentile(x, 75) - np.percentile(x, 25),  # interquartile range\n",
    "        lambda x: np.sum(x < 0),  # negative count\n",
    "        lambda x: np.sum(x > 0),  # positive count\n",
    "        lambda x: stats.skew(x),  # skewness = assymetry\n",
    "        lambda x: stats.kurtosis(x)  # kurtosis\n",
    "    ]\n",
    "    y_train = df_arg['activity'].values\n",
    "    result_columns = ['mean', 'std', 'aad', 'min', 'max', 'range', 'median', 'iqr', 'neg_count', 'pos_count',\n",
    "                      'assymetry', 'kurtosis']\n",
    "    df_arg = get_statistical_measures_df(windowed_data_df=df_arg,\n",
    "                                         functions=functions_list,\n",
    "                                         data_df_columns=sel_columns,\n",
    "                                         result_df_columns=result_columns)\n",
    "    df_arg['activity'] = y_train\n",
    "    \n",
    "    y_test = test_df_arg['activity'].values\n",
    "    test_df_arg = get_statistical_measures_df(windowed_data_df=test_df_arg,\n",
    "                                              functions=functions_list,\n",
    "                                              data_df_columns=sel_columns,\n",
    "                                              result_df_columns=result_columns)\n",
    "    test_df_arg['activity'] = y_test\n",
    "    \n",
    "    # Model training\n",
    "    return model_data_preparation(df_arg=df_arg, test_df_arg=test_df_arg, activity_dict=activity_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf7191e-0d44-4b9e-89be-22271f807e44",
   "metadata": {},
   "source": [
    "## Estimation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6bf39997-26f6-4fe1-b1ee-a4f693cb3641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_usage(df_arg, test_df_arg, activity_dict):\n",
    "    # Measure start time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Call the pipeline function\n",
    "    X_train, y_train, X_valid, y_valid, X_test, y_test = perform_pipeline(df_arg=df_arg, \n",
    "                                                                          test_df_arg=test_df_arg,\n",
    "                                                                          activity_dict=activity_dict,\n",
    "                                                                          time_column='time',\n",
    "                                                                          data_columns = ['accX', 'accY', 'accZ', 'gyrX', 'gyrY', 'gyrZ'],\n",
    "                                                                          important_columns = ['accX', 'accY', 'accZ'])\n",
    "\n",
    "    # Measure end time\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e69eb7a-bf8f-461f-b09c-4d807d165668",
   "metadata": {},
   "source": [
    "# Work with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f0c2f71-f893-4c0d-bc3d-0641d0d45d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 63529 entries, 0 to 63528\n",
      "Data columns (total 9 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   timestamp  63529 non-null  int64  \n",
      " 1   time       63529 non-null  float64\n",
      " 2   accX       63529 non-null  float64\n",
      " 3   accY       63529 non-null  float64\n",
      " 4   accZ       63529 non-null  float64\n",
      " 5   gyrX       63529 non-null  float64\n",
      " 6   gyrY       63529 non-null  float64\n",
      " 7   gyrZ       63529 non-null  float64\n",
      " 8   activity   63529 non-null  object \n",
      "dtypes: float64(7), int64(1), object(1)\n",
      "memory usage: 4.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/Train/Train_activities_1_2023-08-23.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fbca485e-77a4-4315-9220-ed336b773a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12557 entries, 0 to 12556\n",
      "Data columns (total 9 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   timestamp  12557 non-null  int64  \n",
      " 1   time       12557 non-null  float64\n",
      " 2   accX       12557 non-null  float64\n",
      " 3   accY       12557 non-null  float64\n",
      " 4   accZ       12557 non-null  float64\n",
      " 5   gyrX       12557 non-null  float64\n",
      " 6   gyrY       12557 non-null  float64\n",
      " 7   gyrZ       12557 non-null  float64\n",
      " 8   activity   12557 non-null  object \n",
      "dtypes: float64(7), int64(1), object(1)\n",
      "memory usage: 883.0+ KB\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv('data/Test/Test_activities_1_2023-08-23.csv')\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "895ff548-92c8-415d-bf69-cf27c055154c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(X_train) = 1440\n",
      "len(y_train) = 1440\n",
      "len(X_valid) = 361\n",
      "len(y_valid) = 361\n",
      "len(X_test) = 360\n",
      "len(y_test) = 360\n"
     ]
    }
   ],
   "source": [
    "activity_dict = {'Squat': 0, 'Leg land': 1, 'Walk': 2, 'Lateral squat slide': 3, 'Jogging': 4}\n",
    "# Call the pipeline function\n",
    "X_train, y_train, X_valid, y_valid, X_test, y_test = perform_pipeline(df_arg=df, \n",
    "                                                                      test_df_arg=test_df,\n",
    "                                                                      activity_dict=activity_dict,\n",
    "                                                                      time_column='time',\n",
    "                                                                      data_columns = ['accX', 'accY', 'accZ', 'gyrX', 'gyrY', 'gyrZ'],\n",
    "                                                                      important_columns = ['accX', 'accY', 'accZ'])\n",
    "\n",
    "print(f'len(X_train) = {len(X_train)}')\n",
    "print(f'len(y_train) = {len(y_train)}')\n",
    "\n",
    "print(f'len(X_valid) = {len(X_valid)}')\n",
    "print(f'len(y_valid) = {len(y_valid)}')\n",
    "\n",
    "print(f'len(X_test) = {len(X_test)}')\n",
    "print(f'len(y_test) = {len(y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "aa8e3f29-5ea1-41cb-b163-2f9de19cc979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.88413357,  1.03087415,  1.02498789, -0.33719091, -0.74679941,\n",
       "       -0.63565005,  0.534147  , -0.7205218 , -0.72137426, -0.64152274,\n",
       "        0.52381816, -0.69058651,  1.18756413,  1.09661176, -0.06111326,\n",
       "        0.68783474, -0.46314583, -0.00956467,  1.23449886, -0.87305098,\n",
       "       -0.83606425, -0.67057896,  0.60991666, -0.79972501,  0.67877372,\n",
       "        0.76188961,  0.03699406, -0.27521132, -0.66076326, -0.63230541,\n",
       "        0.36452343, -0.60628077, -2.05629075, -0.77707182, -0.1203617 ,\n",
       "        1.11805342,  2.05712079,  0.77761203,  0.1203617 , -1.11677493,\n",
       "        1.22375769,  1.75940193,  0.762303  , -0.74916442, -0.37324391,\n",
       "        0.21846138, -0.01982582, -0.35469597])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f8785eac-7c14-4753-8e57-19a24b29d49b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f815b4f4-5f94-4a73-97ab-414d293207f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.66663815, -0.1063364 ,  1.27585274, -0.01707687, -0.72136628,\n",
       "       -0.56353144,  0.90833894, -0.77168283, -0.71304994, -0.5329888 ,\n",
       "        0.91371841, -0.75043211,  0.49121405,  0.33457735, -0.05435358,\n",
       "        0.83992953, -0.75921519, -0.81228271,  1.67736644, -0.70259715,\n",
       "       -0.66746794, -0.59372015,  0.81090367, -0.79070133, -0.17383551,\n",
       "        0.03574151,  0.06125471, -0.11084289, -0.69525681, -0.44610466,\n",
       "        0.89866244, -0.66566577,  1.53198529,  1.06134563, -0.1203617 ,\n",
       "       -2.21279093, -1.53212041, -1.06067405,  0.1203617 ,  2.21246468,\n",
       "       -0.61211066, -0.65307685,  1.00404051,  3.11954909,  0.62518662,\n",
       "       -0.09388721, -0.27375929,  6.68257847])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3bc1e6f3-4176-4148-877f-0e312dce85f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9fc43acd-e535-4176-a529-db6a6db8a82f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01010963,  0.0844223 ,  1.0921767 ,  0.05928995, -0.67107083,\n",
       "       -0.52586186,  0.53101387, -0.69371078, -0.64381691, -0.55266259,\n",
       "        0.58153447, -0.65778627,  0.78122807,  0.51036363,  0.0174323 ,\n",
       "        0.77127052, -0.68926111, -0.58012654,  0.97274072, -0.76085882,\n",
       "       -0.76791552, -0.58886054,  0.43985424, -0.78522116,  0.30467184,\n",
       "        0.01395932, -0.18753526, -0.01117088, -0.57011924, -0.38160968,\n",
       "        0.58980515, -0.58353149,  0.86748973,  0.08085632, -0.1203617 ,\n",
       "       -1.01368697, -0.93391355, -0.14153101, -1.39999656,  0.88076883,\n",
       "        0.27410654,  0.41683841,  0.76436218, -0.14335533, -0.5247571 ,\n",
       "       -0.43402721, -0.43473181, -0.73622338])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "60040f8b-10f7-4eae-b829-7d9d2b76c6cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d40e3e4-7c65-45dc-b69a-fd757fdc5324",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Investigate optimized pipeline execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a205ded4-291f-4f5d-bf6f-531e3a236b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized pipeline\n",
      "1) time = 8.417 seconds\n",
      "2) time = 8.423 seconds\n",
      "3) time = 8.629 seconds\n",
      "4) time = 8.418 seconds\n",
      "5) time = 8.434 seconds\n",
      "average execution time =  8.464 seconds\n"
     ]
    }
   ],
   "source": [
    "activity_dict = {'Squat': 0, 'Leg land': 1, 'Walk': 2, 'Lateral squat slide': 3, 'Jogging': 4}\n",
    "print(f\"Optimized pipeline\")\n",
    "number_of_experiments = 5\n",
    "time_list = []\n",
    "for i in range(number_of_experiments):\n",
    "    temp_time = get_time_usage(df_arg=df, test_df_arg=test_df, activity_dict=activity_dict)\n",
    "    time_list.append(temp_time)\n",
    "    print(f\"{i+1}) time = {temp_time:.3f} seconds\")\n",
    "    \n",
    "print(f\"average execution time = {sum(time_list) / len(time_list): .3f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
